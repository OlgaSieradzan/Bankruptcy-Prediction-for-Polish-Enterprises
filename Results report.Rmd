---
title: "Bankruptcy Prediction for Polish Enterprises"
author: "Olga Sieradzan, Jakub Wasiczek"
date: "2025-04-10"
output: 
  html_document:
    toc: true
    toc_float: 
      smooth_scroll: true
    df_print: paged

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(foreign)
library(dplyr)
library(cowplot)
library(ggplot2)
library(readr)
library(readxl)

y5 <- read.csv("5year.arff", header = TRUE, comment.char = "@")
colnames(y5) <- paste0("V", 1:65)

```

***

# Introduction

***

Corporate bankruptcy prediction is a critical area of financial analysis, with significant implications for investors, creditors, and policymakers. Accurate early warning systems can help mitigate risks, optimize decision-making, and ensure economic stability. Over the years, various statistical and machine learning models have been developed to assess the financial health of companies and predict their likelihood of failure.

This project explores and compares multiple predictive models for bankruptcy assessment, ranging from traditional econometric approaches to modern machine learning techniques. We evaluate the effectiveness of models such as the Altman Z-score (a classic discriminant analysis tool), legendary Tutson model, logistic regression, decision trees, random forests, and other advanced classifiers. 

The aim of this project is to assest numerous models predictions on bancrupcy for polish companies between year 2000- 2013.

***

# Dataset

***

The dataset consist 64 variables conected to financial situation of each company and corresponding class label that indicates bankruptcy status after 1 year. The data contains 5910 instances (financial statements), 410 represents bankrupted companies, 5500 firms that did not bankrupt in the forecasting period.

There were some requirements set for data:

* No missing data - all of the rows with data gaps were deleted

* No high correlation between variables - variables with correlation hiher than 70% were deleted

* No data showing little variation - variables with variablility lower than 10% were deleted

After process of cleaning the traing set consist **23 variables and 140 observation (70 bancrupted and 70 not bancrupted)**, and the test set consist **23 variables and 64 observation (32 bancrupted and 32 not bancrupted)**

The 23 left variables:

V2 - total liabilities / total assets
V3 - working capital / total assets
V4 - current assets / short-term liabilities
V9 - sales / total assets
V12 - gross profit / short-term liabilities
V13 - (gross profit + depreciation) / sales
V15 - (total liabilities * 365) / (gross profit + depreciation)
V20 - (inventory * 365) / sales
V21 - sales (n) / sales (n-1)
V24 - gross profit (in 3 years) / total assets
V28 - working capital / fixed assets
V29 - logarithm of total assets
V30 - (total liabilities - cash) / sales
V33 - operating expenses / short-term liabilities
V37 - (current assets - inventories) / long-term liabilities
V43 - rotation receivables + inventory turnover in days
V55 - working capital
V57 - (current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)
V59 - long-term liabilities / equity
V60 - sales / inventory
V61 - sales / receivables
V64 - sales / fixed assets



***

# Altman model

***

The Altman Z-Score is a classic financial model developed by Edward Altman in 1968 to predict the likelihood of corporate bankruptcy. It combines multiple financial ratios (such as profitability, leverage, liquidity, and efficiency) into a single discriminant score using a weighted formula. The original version for manufacturing firms is calculated as:

$$
Z = 1.2X_1 + 1.4X_2 + 3.3X_3 + 0.6X_4 + 0.99X_5
$$

  represent ratios like working capital/total assets, retained earnings/total assets, EBIT/total assets, market value equity/total liabilities, and sales/total assets. Based on the Z-score, companies are classified as:

Safe (Z > 2.99) – Low bankruptcy risk.

Grey Zone (1.81 ≤ Z ≤ 2.99) – Moderate risk.

Distress (Z < 1.81) – High bankruptcy risk.

The model is widely used due to its simplicity and empirical robustness.


***

# Tutson model

***

The Tutsona Model is an analytical tool researchers developed using linear discriminant analysis (LDA) tailored specifically for Polish companies. The model’s name is no accident—it pays tribute to a true master of motivation, an undisputed expert in stress management, and an unrivaled specialist in... sunbeam naps. Yes, I’m talking about Tutek, a chonky cat who not only provided moral support with his presence but also inspired groundbreaking solutions in finance and stock market analysis.

Thanks to his unwavering belief in success (and his knack for sitting on my keyboard at critical moments), the Tutsona Model became a symbol of combining academic precision with feline wisdom. It’s more than just an analytical tool—it’s a tribute to a true legend, proving that even the toughest challenges can be overcome with a bit of cat philosophy and a good nap.

```{r}
knitr::include_graphics("C:/Users/olgas/OneDrive/Documents/GitHub/Quantative-risk-models-summit/WhatsApp Image 2025-03-06 at 09.54.51_3e79e9e8.jpg")
```


The downfall of the Tutson model can be easily explained. Linear Discriminant Analysis (LDA) is a method that relies on several statistical assumptions—none of which were met by our dataset. First and foremost, LDA assumes that the predictor variables follow a normal distribution. In addition, it requires that all classes have equal covariance matrices. These assumptions are difficult to uphold in real-world data, which is one of the reasons behind the declining popularity of LDA in practical modeling applications.

Over the past decade, a number of more flexible algorithms have been introduced and widely adopted in the financial sector. These include methods such as Random Forests, Gradient Boosting Machines, and Neural Networks. These newer approaches typically make fewer assumptions about the data and are better equipped to handle complex, non-linear relationships.

But are they truly that much better? While they often outperform traditional methods like LDA in terms of predictive accuracy, they come with their own trade-offs—such as increased computational complexity, reduced interpretability, and a higher risk of overfitting if not properly tuned. Ultimately, the choice of algorithm should depend on the specific context, data characteristics, and the balance between accuracy and explainability required by the stakeholders.


*** 

# Quadratic Discriminant Analysis

Quadratic Discriminant Analysis (QDA) is an extension of LDA that relaxes the assumption of equal class covariances. While this makes QDA more flexible, it comes at a cost: instead of estimating a single covariance matrix with $p(p+1)/2$ parameters (where *p* is the number of predictors), QDA requires estimating a separate covariance matrix for each class, resulting in $K*p(p+1)/2$ parameters. This significantly increases the variance of the estimators, which is why QDA isn't always preferred over LDA — especially with limited data.


***  

# Naive Bayes  
 
Naive Bayes makes a strong assumption that all predictors are independent. This means the joint probability of the predictor vector *X* belonging to class *k* is simply the product of the individual predictor densities. In practice, this assumption rarely holds—especially in our case, where many financial ratios are derived from overlapping data. However, since we initially removed highly correlated variables, Naive Bayes may still perform reasonably well. 

***

# Logistic Regression

***

Logistic regression is a statistical method used to predict binary outcomes (e.g., bankruptcy vs. solvency) by modeling the probability of an event occurring. Unlike linear regression, it uses the logistic function (sigmoid curve) to output values between 0 and 1, interpreted as the likelihood of default. Key features include:

* Input: Financial ratios (e.g., liquidity, leverage).

* Output: Probability (e.g., "0.8" = 80% chance of bankruptcy).

* Advantages: Simple, interpretable (coefficients show variable impact), and works well for linearly separable data.

For example, a model might reveal that high debt-to-equity ratios significantly increase bankruptcy risk. While less flexible than ML algorithms, it remains a benchmark tool for financial distress analysis. In this study, the treshold for assesing brancrupcy is: 50%> - bancrupted, 50%=< - not bancrupted. Manipulation of this treshold can give intresting results in Accuracy or False Negative rates 

Modyfikacja treshoholdu 

***

# XGBoost (Extreme Gradient Boosting)

***

XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that excels at predicting bankruptcy by combining multiple weak "decision trees" into a highly accurate ensemble model. Model iteratively corrects errors from previous trees, focusing on "hard-to-predict" cases. Additionaly it Handles non-linear patterns, missing data, and imbalanced datasets (common in bankruptcy cases). Built-in regularization prevents overfitting, making it robust even with small financial datasets.

For example, XGBoost might detect complex interactions between profitability ratios and market conditions that simpler models miss. While less interpretable than logistic regression, its performance often dominates in predictive accuracy.

In this research 100 rounds of "boosting" were performed. Reserchers belive this number performed the best out of all tested and prevent overfiting of model. Additionally, binary  function of aim was used. 

***

# Random Forest Model

***

The Random Forest model is an ensemble machine learning method that builds multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. Model trains hundreds of decision trees on random subsets of data (bagging) and features (feature randomness). Combines their votes (for classification) or averages their outputs (for regression). This model handles non-linear relationships and high-dimensional data (e.g., dozens of financial ratios),althought it's less interpretable than single decision trees, it's more accurate.


***

# Model comparasion

***

***

# Large traing set senerio

***

Beyond traning and assesing models on mentioned dataset, researchers created much bigger dataset to perform additional analysys. Large traning set consist over 1000 rows, and new column, with data about . With this much data, not all of the models were compatible. Only LDA , Random Forest and XGBoost were succesfull wiith training on this amaout of information. 

***

# Conclusions

***




