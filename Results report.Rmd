---
title: "Bankruptcy Prediction for Polish Enterprises"
author: "Olga Sieradzan, Jakub Wasiczek"
date: "2025-04-10"
output: 
  html_document:
    toc: true
    toc_float: 
      smooth_scroll: true
    df_print: paged

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(foreign)
library(dplyr)
library(cowplot)
library(ggplot2)
library(readr)
library(readxl)

y5 <- read.csv("5year.arff", header = TRUE, comment.char = "@")
colnames(y5) <- paste0("V", 1:65)

Rates <- read_excel("Results_sheets.xlsx", 
    sheet = "Rates")

Altman <- read_excel("Results_sheets.xlsx", 
    sheet = "LDA")

Matrixes <- read_excel("Results_sheets.xlsx", 
    sheet = "Matrix")

```

***

# Introduction

***

Corporate bankruptcy prediction is a critical area of financial analysis, with significant implications for investors, creditors, and policymakers. Accurate early warning systems can help mitigate risks, optimize decision-making, and ensure economic stability. Over the years, various statistical and machine learning models have been developed to assess the financial health of companies and predict their likelihood of failure.

This project explores and compares multiple predictive models for bankruptcy assessment, ranging from traditional econometric approaches to modern machine learning techniques. We evaluate the effectiveness of models such as the Altman Z-score (a classic discriminant analysis tool), legendary Tutson model, logistic regression, decision trees, random forests, and other advanced classifiers. 

The aim of this project is to assest numerous models predictions on bancrupcy for polish companies between year 2000- 2013.

***

# Dataset

***

The dataset consist 64 variables conected to financial situation of each company and corresponding class label that indicates bankruptcy status after 1 year. The data contains 5910 instances (financial statements), 410 represents bankrupted companies, 5500 firms that did not bankrupt in the forecasting period.

There were some requirements set for data:

* No missing data - all of the rows with data gaps were deleted

* No high correlation between variables - variables with correlation hiher than 70% were deleted

* No data showing little variation - variables with variablility lower than 10% were deleted

After process of cleaning the traing set consist **23 variables and 140 observation (70 bancrupted and 70 not bancrupted)**, and the test set consist **23 variables and 64 observation (32 bancrupted and 32 not bancrupted)**

The 23 left variables:

* V2 - total liabilities / total assets

* V3 - working capital / total assets

* V4 - current assets / short-term liabilities

* V9 - sales / total assets

* V12 - gross profit / short-term liabilities

* V13 - (gross profit + depreciation) / sales

* V15 - (total liabilities * 365) / (gross profit + depreciation)

* V20 - (inventory * 365) / sales

* V21 - sales (n) / sales (n-1)

* V24 - gross profit (in 3 years) / total assets

* V28 - working capital / fixed assets

* V29 - logarithm of total assets

* V30 - (total liabilities - cash) / sales

* V33 - operating expenses / short-term liabilities

* V37 - (current assets - inventories) / long-term liabilities

* V43 - rotation receivables + inventory turnover in days

* V55 - working capital

* V57 - (current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)

* V59 - long-term liabilities / equity

* V60 - sales / inventory

* V61 - sales / receivables

* V64 - sales / fixed assets

While assesing models researchers used 2 quantatives ; model's accuracy and false negative rate - how many % of bancruts were not discovered.



***

# Altman model

***

The Altman Z-Score is a classic financial model developed by Edward Altman in 1968 to predict the likelihood of corporate bankruptcy. It combines multiple financial ratios (such as profitability, leverage, liquidity, and efficiency) into a single discriminant score using a weighted formula. The original version for manufacturing firms is calculated as:

$$
Z = 1.2X_1 + 1.4X_2 + 3.3X_3 + 0.6X_4 + 0.99X_5
$$

  represent ratios like working capital/total assets, retained earnings/total assets, EBIT/total assets, market value equity/total liabilities, and sales/total assets. Based on the Z-score, companies are classified as:

* Safe (Z > 2.99) – Low bankruptcy risk.

* Grey Zone (1.81 ≤ Z ≤ 2.99) – Moderate risk.

* Distress (Z < 1.81) – High bankruptcy risk.

***

**Results**

***

The table consist information about model's performance.
```{r}
Matrixes <-Matrixes %>%
  mutate(Actual = ifelse(Actual == "1", "Bancrupted", "Alive")) %>%
  mutate(Predicted = ifelse(Predicted == "1", "Bancrupted", "Alive"))

colnames(Rates) <- c("Accuracy", "False Negative Rate", "Method")
Altman
```

A lot of companies ended up in the grey area, which is a big disadvantage of this model. In the end, model should give as precised answer as possible. 

```{r}
Rates %>%
  filter(Method == "altman")
```

If it comes to model's accuracy, it is really low - only 55%. The False negative rate on the other hand is a little bit over 35%, which is quite good, only 35% of bancrupted comapnies were not spotted. 

Altman’s model, although brilliant for its time, has proven to be insufficient for the current Polish market. This may be due to the overly general nature of the model—the diagnostic variables and coefficients selected by Altman are not necessarily well-suited to companies across all markets, as they were estimated based on a very limited sample of firms from a single market. Moreover, it is worth noting that the data on which the model was built can now be considered outdated and no longer reflective of the realities faced by modern companies.

***

# Tutson model

***

The Tutson Model is an analytical tool researchers developed using linear discriminant analysis (LDA) tailored specifically for Polish companies. The model’s name is no accident—it pays tribute to a true master of motivation, an undisputed expert in stress management, and an unrivaled specialist in... sunbeam naps. Yes, I’m talking about Tutek, a chonky cat who not only provided moral support with his presence but also inspired groundbreaking solutions in finance and stock market analysis.

Thanks to his unwavering belief in success (and his knack for sitting on my keyboard at critical moments), the Tutsona Model became a symbol of combining academic precision with feline wisdom. It’s more than just an analytical tool—it’s a tribute to a true legend, proving that even the toughest challenges can be overcome with a bit of cat philosophy and a good nap.

***

**Results**

***

```{r}
Matrixes %>%
  filter(method == "tutson")
```

```{r}
Rates %>%
  filter(Method == "tutson")
```
Accuracy achieved by LDA model made specifically for Polish market, is visibly higher than Altman's model outcome. Unfortunately, it's still low, and  False Negative rate is beyond disappointing. 

The downfall of the Tutson model can be easily explained. Linear Discriminant Analysis (LDA) is a method that relies on several statistical assumptions—none of which were met by our dataset. First and foremost, LDA assumes that the predictor variables follow a normal distribution. In addition, it requires that all classes have equal covariance matrices. These assumptions are difficult to uphold in real-world data, which is one of the reasons behind the declining popularity of LDA in practical modeling applications.

Over the past decade, a number of more flexible algorithms have been introduced and widely adopted in the financial sector. These include methods such as Random Forests, Gradient Boosting Machines, and Neural Networks. These newer approaches typically make fewer assumptions about the data and are better equipped to handle complex, non-linear relationships.

But are they truly that much better? While they often outperform traditional methods like LDA in terms of predictive accuracy, they come with their own trade-offs—such as increased computational complexity, reduced interpretability, and a higher risk of overfitting if not properly tuned. Ultimately, the choice of algorithm should depend on the specific context, data characteristics, and the balance between accuracy and explainability required by the stakeholders.


*** 

# Quadratic Discriminant Analysis

Quadratic Discriminant Analysis (QDA) is an extension of LDA that relaxes the assumption of equal class covariances. While this makes QDA more flexible, it comes at a cost: instead of estimating a single covariance matrix with $p(p+1)/2$ parameters (where *p* is the number of predictors), QDA requires estimating a separate covariance matrix for each class, resulting in $K*p(p+1)/2$ parameters. This significantly increases the variance of the estimators, which is why QDA isn't always preferred over LDA — especially with limited data.

***

**Results**

***

```{r}
Matrixes %>%
  filter(method == "qda")
```

```{r}
Rates %>%
  filter(Method == "qda")
```
If it comes to accuracy, it improved significantly, compared to previous two results. Result equal to 75% seems reasonable and quite satisfying. Additionally, QDA achived lowest False Negative Rate out of all already researched methods.  Relaxing of the assumption of equal class covariances, gave very promising result.

***  

# Naive Bayes  
 
Naive Bayes makes a strong assumption that all predictors are independent. This means the joint probability of the predictor vector *X* belonging to class *k* is simply the product of the individual predictor densities. In practice, this assumption rarely holds—especially in our case, where many financial ratios are derived from overlapping data. However, since we initially removed highly correlated variables, Naive Bayes may still perform reasonably well. 

***

**Results**

***

```{r}
Matrixes %>%
  filter(method == "naive bayes")
```

```{r}
Rates %>%
  filter(Method == "naive bayes")
```
Al thought the assumption was not met, naive baias performed quite well, better then the LDA but slighty worse than QDA model. The False negative rate thought is still disappointing. Naive Bayes has not occurred as groundbreaking method, and the exploration for better models shall not stop.

***

# Logistic Regression

***

Logistic regression is a statistical method used to predict binary outcomes (e.g., bankruptcy vs. solvency) by modeling the probability of an event occurring. Unlike linear regression, it uses the logistic function (sigmoid curve) to output values between 0 and 1, interpreted as the likelihood of default. While less flexible than ML algorithms, it remains a benchmark tool for financial distress analysis. In this study, the treshold for assesing brancrupcy is: 50%> - bancrupted, 50%=< - not bancrupted. Manipulation of this treshold can give intresting results in Accuracy or False Negative rates 

***

**Results - 50% threshold**

***

```{r}
Matrixes %>%
  filter(method == "logistic regression")
```

```{r}
Rates %>%
  filter(Method == "logistic regression")
```

Logistic regression definitely gave some different and intresting results. Firstly, accuracy is pretty high which is satisfaing, but the really good information is a very low False Negative rate, which is under 22%. It's the first model to predict less companies alive while bankrupted, than the other way around (bancrupted while alive). Logistic regression achived very promising results.

***

**Results - 80% threshold**

***

Managing verges of threshold is additional feature that logistic regression is offering. Firstly, it is research, how increasing threshold to as much as 80% will affect model's rates. 

```{r}
Rates %>%
  filter(Method == "lr08")
```

Results are of course dissapointing, but it was easy to predict. More intresting results are obtaing by lowering the threshold to jus 10%.

***

**Results - 10% threshold**

***

Lowering threshold may occure very usefull, because model will be 'more carefull'.

```{r}
Rates %>%
  filter(Method == "lr01")
```

As assumed, while accuracy might not be wonderful, the False Negative Rate is. Logistic Regression with 10% threshold has come out as the best in detecting bankrupted companies, which is a very valuable feature. For the bank, it will be more important not to lend money to soon bankrupted comapny than not to lend money to company doinbg okay.


***

# XGBoost (Extreme Gradient Boosting)

***

XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that excels at predicting bankruptcy by combining multiple weak "decision trees" into a highly accurate ensemble model. Model iteratively corrects errors from previous trees, focusing on "hard-to-predict" cases. Additionaly it Handles non-linear patterns, missing data, and imbalanced datasets (common in bankruptcy cases). Built-in regularization prevents overfitting, making it robust even with small financial datasets.

For example, XGBoost might detect complex interactions between profitability ratios and market conditions that simpler models miss. While less interpretable than logistic regression, its performance often dominates in predictive accuracy.

In this research 200 rounds of "boosting" were performed. Reserchers belive this number performed the best out of all tested and prevent overfiting of model. Additionally, binary  function of aim was used. 

***

# Random Forest Model

***

The Random Forest model is an ensemble machine learning method that builds multiple decision trees and merges their predictions to improve accuracy and reduce overfitting. Model trains hundreds of decision trees on random subsets of data (bagging) and features (feature randomness). Combines their votes (for classification) or averages their outputs (for regression). This model handles non-linear relationships and high-dimensional data (e.g., dozens of financial ratios),althought it's less interpretable than single decision trees, it's more accurate.


***

# Model comparasion

***

***

# Large traing set senerio

***

Since the project was initially based on a limited dataset—focusing only on companies one year before bankruptcy—we now aim to test whether including a new variable, "Number of years before bankruptcy," can improve prediction performance. Beyond traning and assesing models on mentioned dataset, researchers created much bigger dataset to perform additional analysys. Large traning set consist over 1000 rows, and new column. With this much data, not all of the models were compatible. Only LDA , Random Forest and XGBoost were succesfull wiith training on this amaout of information.  By expanding the dataset in this way, we increase the sample size, which gives us reason to believe that the models may achieve better results.  

***

# Conclusions

***




